#+TITLE:     大数据环境下信息抽取模板自动聚类与发现
#+AUTHOR:    计92 丘骏鹏 2009011282
#+EMAIL:     qjp-ch-mail@163.com
#+DATE:      指导老师：朱小燕~郝宇
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:
#+LINK_HOME:
#+XSLT:

#+STARTUP: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [11pt,presentation]
#+BEAMER_FRAME_LEVEL: 2
#+BEAMER_HEADER_EXTRA: \usetheme{default}\usecolortheme{default}
#+BEAMER_HEADER_EXTRA: \usepackage{listings}\usepackage{fontspec}\usepackage{xunicode}\usepackage{xltxtra}\usepackage{xeCJK}
#+BEAMER_HEADER_EXTRA: \setmainfont{Times New Roman}\setmonofont{Courier New}\setCJKmainfont[BoldFont=YouYuan]{SimSun}\setCJKfamilyfont{song}{SimSun}\setCJKfamilyfont{msyh}{微软雅黑}\setCJKfamilyfont{fs}{FangSong}
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_envargs(Env Args) %4BEAMER_col(Col) %8BEAMER_extra(Extra)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 :ETC
#+LATEX_HEADER: \AtBeginSection[]{\begin{frame}<beamer>\frametitle{提纲}\tableofcontents[currentsection]\end{frame}}

\begin{frame}<beamer>\frametitle{提纲}\tableofcontents\end{frame}
* 选题回顾
** 背景
- 已经获取到海量的新闻、博客、论坛等网页原始数据，需要从中提取结构化的信息
- 做法：从已有数据中抽取模板，利用模板去抽取相似网页中的信息。
- 目标：提取结构化信息，如新闻中的标题和正文，博客的标题和内容等，存储成以下格式，
  用于后续的处理。\tiny
   #+begin_src nxml
     <document>
       <news>
         <title>foobar</title>
         <content>blablabla</content>
       </news>
     </document>
   #+end_src

** 输入
*** 文档集合
#+begin_src html
  <html>                |  <html>
    <body>              |    <body>
      <h1>Title1</h1>   |      <h1>Title2</h1>
      <p>Content1</p>   |      <p>Content2</p>
    </body>             |    </body>
  </html>               |  </html>
------------------------|--------------------------
  <html>                |  <html>
    <body>              |    <body>
      <div>             |      <div>
        <div>foo1</div> |        <div>foo2</div>
        <div>bar1</div> |        <div>bar2</div>
      </div>            |      </div>
    </body>             |    </body>
  </html>               |  </html>
#+end_src

** 输出
*** 抽取的模板1                                                       :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_envargs: C[t]
    :END:
#+begin_src html
  <html>
    <body>
      <h1>?</h1>
      <p>?</p>
    </body>
  </html>
#+end_src
*** 抽取的模板2                                                       :BMCOL:
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_envargs: C[t]
    :END:
#+begin_src html
  <html>
    <body>
      <div>
        <div>?</div>
        <div>?</div>
      </div>
    </body>
  </html>
#+end_src

* 系统设计与实现
** 框架
***                                                   :BMCOL:B_ignoreheading:
    :PROPERTIES:
    :BEAMER_col: 0.3
    :BEAMER_env: ignoreheading
    :BEAMER_envargs: C[t]
    :END:
整体框架示意图
***                                                   :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_env: ignoreheading
    :BEAMER_col: 0.7

    :END:
    #+CAPTION: framework
    #+LABEL: fig:1
    #+ATTR_LaTeX: width=20em,angle=0
    [[./framework.png]]
#+begin_src dot :file framework.png :exports none
  digraph G {
          subgraph cluster0 {
                  label="主体框架";
                  node[shape=box, style="filled,rounded", color=darkturquoise, fontcolor=white];
                  A[label="网页过滤"];
                  C[label="网页聚类"];
                  D[label="网页模板提取"];
                  node[shape=ellipse, style="filled,rounded",color=tomato];
                  B[label="详细页集合"];
                  A->B->C->D;
          }
          node[shape=ellipse, style="filled,rounded",color=gray, fontcolor=white];
          input[label="输入网页集合"];
          output[label="抽取结构化信息",shape=box,color=gray];
          input->A;
          D->output;
          node[shape=plaintext,color=white,fontcolor=black];
          E[label="计算HTML文档相似度"];
          E->C;
          F[label="无监督或半监督方法"];
          F->D;
  }
#+end_src

#+RESULTS:
[[file:framework.png]]

** 系统实现概况
*** 主要模块实现
    - [X] 整体框架搭建
    - [X] 网页过滤
    - [X] 网页聚类
    - [ ] 模板抽取
*** 进度对比
**** 开题报告
      - 5-8周：网页过滤，网页聚类和网页模板提取模块初步实现
      - 9-12周：算法修正，系统改进，结果分析
**** 实际进度
      - 5-8周：初步的模板提取模块尚未完全实现
      - 9-12周：由于计算速度瓶颈，目前已进行了算法的优化


** 搭建系统框架
    - 实现语言：Java+Scala
    - 考虑到代码的重用性，采用了许多工业界广泛应用的第三方库：
      1. [[http://site.icu-project.org/][icu4j]] : 用于检测网页字符编码
      2. [[http://jsoup.org][Jsoup]] : HTML Parser。可以自定义Visitor来访问树的节点。
      3. [[https://github.com/twitter/util][util-logging]] : twitter包装的java.util.logging库，用于日志系统
      4. [[https://github.com/typesafehub/config][typesafe's config]] : 完成配置文件读取
      5. [[http://akka.io][akka]] : Java & Scala的Actor模型库

** 实验数据
*** 实验数据统计
    |          |  blog |  news |  other |
    |----------+-------+-------+--------|
    | 文件个数 | 59998 | 81561 | 183635 |
    | 总大小   |  5.4G |  7.9G |    18G |

    主要针对blog数据做了一些实验
** 系统设计（1）
*** 网页过滤模块
- 新浪博客的目录页和详细页可以用URL区分。比如某个博主的目录页为
  \tiny
  #+begin_example
  http://blog.sina.com.cn/u/1439351555
  #+end_example
  \normalsize
  他的某篇文章的URL格式为
  \tiny
  #+begin_example
  http://blog.sina.com.cn/s/blog_55cac30301016yb1.html
  #+end_example
  \normalsize
  因此对于博客数据可以用URL正则进行过滤
- blog文档集合中目录页文件数为23430，详细页文件数为36568。

** 系统设计（2）
*** 预处理
- 去除空行、标签属性值、文本 =<#text>= 和 =CDATA= 数据以及无用标签
  \tiny
  #+begin_example
  <script>, <link>, <style>, <br>, <img>, <em>
  #+end_example
  \normalsize
- 将树结构平坦化，降低计算复杂度。通过前序遍历 =Dom Tree= 得到tag序列：
  \tiny
  #+begin_example 
  <html>
    <body>
      <div>
        <p></p>
        <a></a>         
      </div>           
      <div></div>      
    </body>
  </html>
  #+end_example
  \normalsize
  转化成:
  \tiny
  #+begin_example
  <html><body><div><p></p><a></a></div><div></div></body></html>
  #+end_example 
  

** 系统设计（3）
*** 相似度计算
为了方便计算以及后续的模板的抽取，采用LCS作为计算相似度的基础
#+BEGIN_LaTeX
\begin{eqnarray*}
  c(i)(j) =
  \begin{cases}
    0 & i = 0,\: j = 0\\
    c(i-1)(j-1) + 1 & i,\: j > 0, x_i=y_j\\
    \max(c(i)(j-1), c(i-1)(j)) & i, j > 0,\: x_i \ne y_j
  \end{cases}
\end{eqnarray*}
#+END_LaTeX
*** Longest Common Tag Subsequence
\[
d_{LCTS}(D_1,D_2)=1-\frac{|lcts(D_1,D_2)|}{\max(|D_1|,|D_2|)}
\]

** 重复记录的处理
   - 网页中含有部分重复元素，这些部分是由网站后台动态生成的(Python Django)：
     #+begin_src python
        {% for file in file_list %}
          <li>{{file}}</li>
        {% endfor %}
     #+end_src
   - 简单的方法：去掉这些标签，在比较过程中不予考虑。但这种方法无法处理更复杂的
     情况。如一个 =<div>= 标签下的子树（Data Record）。
   - 后缀树（SuffixTree）
     - Trie的一个变种，可以快速找到字符串中的重复子串
     - 快速算法可以在\(O(n)\) 时间内构建: \\\footnotesize\em
       Ukkonen, Esko. "On-line construction of suffix trees." Algorithmica 14.3
       (1995): 249-260.
   - 由于采用前序遍历，可以保证子树在序列上是连续的

** 后缀树
   - 对于字符串mississippi：
     \tiny
     #+begin_example
     T1  = mississippi        tree-->|---mississippi                T1
     T2  = ississippi                |                             
     T3  = ssissippi                 |---i-->|---ssi-->|---ssippi   T2
     T4  = sissippi                  |       |         |           
     T5  = issippi                   |       |         |---ppi      T5
     T6  = ssippi                    |       |                     
     T7  = sippi         =>          |       |---ppi                T8
     T8  = ippi                      |                             
     T9  = ppi                       |---s-->|---si-->|---ssippi    T3
     T10 = pi                        |       |        |            
     T11 = i                         |       |        |---ppi       T6
                                     |       |                     
                                     |       |---i-->|---ssippi     T4
                                     |               |             
                                     |               |---ppi        T7
                                     |                             
                                     |---p-->|---pi                 T9
                                             |                     
                                             |---i                  T10
                                                                                  
     #+end_example
     \normalsize
   - 任一到内部节点的路径都是字符串中重复的子串


** 聚类算法
*** 实现了一个简单的层次聚类算法
*** 过程
    - 每个文档开始时单独为一类，并作为该类的中心点
    - 选择中心点距离最近的两个类进行合并
    - 更新类中心点：选择距离其他点距离之和最小的点作为类的新中心点，重复以上过程
*** 算法特点
    - 只需要计算一次文档集合相互之间的相似度
    - 阈值较难设置
* 系统优化
** 动机
1. LCS的动态规划算法的时间复杂度为\(O(mn)\) ，空间复杂度也是\(O(mn)\)
2. 文档数很大，运行时需载入内存，需要尽量减少空间复杂度
3. 假设每运行一次算法的时间为t，以最小的文档集合blog为输入（数目约为60000篇），则
   计算文档集合中两两之间距离的总时间约为：
   \[
   \frac{60000^2}{2 * 3600}*t=5*10^5*t
   \]
   取\(t=0.001s\)，则总时间为\(5*10^5*0.001=500h\)。

** 优化空间
   - 动态规划原理式
     #+BEGIN_LaTeX
     \begin{eqnarray*}
       c(i)(j) =
       \begin{cases}
         0 & i = 0,\: j = 0\\
         c(i-1)(j-1) + 1 & i,\: j > 0, x_i=y_j\\
         \max(c(i)(j-1), c(i-1)(j)) & i, j > 0,\: x_i \ne y_j
       \end{cases}
     \end{eqnarray*}
     #+END_LaTeX
     以行优先遍历为例：实际上我们在计算每一个点的值时，依赖的信息只包括这一行之
     前已计算出的点和前一行的点，所以只需要两个一维数组即可。空间复杂度降低为
     \(O(n)\)。

** 优化时间
   - 减小运行时间的有效办法是压缩tag序列长度。
     1. 预处理已经去掉了很多无用标签
     2. =<tagName></tagname>= 可以用 =(tagName, depth)= 来表示,相当于将HTML转换为
        S-expression。\footnotesize
        #+begin_example
        <html>                          <=>  (html
         <body>                         <=>    (body
          <div>                         <=>      (div
           <p></p></div></body></html>  <=>        (p))))
        #+end_example
        \normalsize
   - 用途：由于大部分的标签都是成对的，因此这样大概可以减少一半的tag序列长度。
   - 标签比较时加入深度信息：
     \footnotesize
     #+begin_example
     node1.tagName = node2.tagName && node1.depth = node2.depth
     #+end_example
     \normalsize
** 优化计算方式（1）
   - 在以上优化的基础上，两两之间进行一次计算需要的时间为\(0.001\sim 0.002s\)。
   - 之前已经计算过，在\(t=0.001s\)的情况下，计算一次blog集合中所有文档相互之间
     的距离需要500小时。
   - 优化计算方式：采用多线程进行计算。
** 优化计算方式（2）
*** 采用Actor库进行实现
    - 一种并行计算的模型，每个Actor是完全独立的，相互间采用异步、非阻塞的消息传递
      进行通信
    - 优点：可以避免使用全局状态、锁、信号量等一些低级的同步原语；有封装好的线程
      调度算法，不需要手动对线程进行管理，简化任务的分割。
*** 具体实现
    :PROPERTIES:
    :END:
**** 将该区域用等距的横线和纵线分割，然后将这些区域通过调度器分发给每个可用的Actor进行计算。调度算法采用简单的Round-Robin。
****                                                  :B_ignoreheading:BMCOL:
    :PROPERTIES:
    :BEAMER_env: ignoreheading
    :END:
#+CAPTION: 示意图
#+LABEL: fig:1
#+ATTR_LaTeX: width=0.3\textwidth,angle=0
[[./图片1.jpg]]



** 优化距离计算
   - 考虑深度的影响，离根节点越近的标签权重越大
   - 修改LCS算法，\(f(x)\)是一个与当前节点深度\(x\)有关的函数：
     #+BEGIN_LaTeX
     \begin{eqnarray*}
       c(i)(j) =
       \begin{cases}
         0 & i = 0,\: j = 0\\
         c(i-1)(j-1) + f(x_i.depth) & i,\: j > 0, x_i=y_j\\
         \max(c(i)(j-1), c(i-1)(j)) & i, j > 0,\: x_i \ne y_j
       \end{cases}
     \end{eqnarray*}
     #+END_LaTeX
   - 同时修改距离计算公式
     \[
     d_{LCTS}(D_1,D_2)=1-\frac{|lcts(D_1,D_2)|}{\max(\sum\limits_{n\in
     D_1}{f(n.depth)},\sum\limits_{n\in D_2}{f(n.depth)})}
     \]
         
* 初步结果
** 实验设置
*** 在实验室的服务器上进行实验，机器配置为16个逻辑CPU+24G内存
*** 由于以上限制，目前在小数据量上做实验：从blog中抽取出了1000个文档作为实验的文档集合
** 实验结果
  - 直接聚类：在阈值为0.3的情况下，所有文档聚成一类，通过手工可以大致确定正确性
  - 验证可行性：加入噪音
    - 在详细页文档中加入目录页、404错误页等噪音
    - 聚类结果：在阈值为0.3的情况下，文档被聚成3类，分别是详细页，目录页和404错
      误页
  - 初步分析：
    - 阈值设置：目前暂无法确定合适阈值将两个文档分为不同的两类
    - 评价：需要根据最后的模板抽取结果来判断

* 后期工作
** 后期工作目标
*** 实现模板抽取
    - 利用计算出的公共字串及tag的深度信息反向构建出树结构，作为该类的模板
    - 采取少量标注进行半监督学习
*** 新文档分类
    - 归为已有的一类，利用该类的模板抽取文档内容
    - 归为新的类，计算新的模板
*** 结果评价
    - 根据模板抽取结果，调整实验参数
